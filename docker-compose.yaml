# https://github.com/adamksiezyk/data-science-workbench/blob/mlflow-local-llm/docker-compose.yml
version: '3.8'
services:
  app:
    # image: 
    build: ./llm_server
    ports:
      - 8000:8000
    volumes:
      - ./llm_server:/app/
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 3
    restart: always
    environment:
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI}
      - OLLAMA_URI=${OLLAMA_URI}
      - OLLAMA_MODEL=${OLLAMA_MODEL}
    depends_on:
      - llm
      - mlflow-server
    networks:
      - ollama-docker

  llm:
    image: ollama/ollama:latest
    ports:
      - ${OLLAMA_PORT}:${OLLAMA_PORT}
    volumes:
      - ./data/llm:/root/.ollama
    container_name: ollama
    entrypoint: ["/bin/ollama"]
    command: >
      serve &&
      /bin/ollama run llama3
    pull_policy: always
    tty: true
    restart: always
    networks:
      - ollama-docker
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  mlflow-deployments:
    image: mlflow
    build: ./mlflow
    command: "mlflow deployments start-server --config-path config.yaml --host 0.0.0.0 --port 5000 --workers 1"
    expose:
      - 5000
    volumes:
      - ./mlflow/config.yaml:/app/config.yaml:ro
    networks:
      - ollama-docker

  mlflow-server:
    image: mlflow
    build: ./mlflow
    command: 'mlflow server --host 0.0.0.0 --port ${MLFLOW_SERVER_PORT} --serve-artifacts --backend-store-uri sqlite:////app/db.sqlite --gunicorn-opts "--timeout=120"'
    ports:
      - ${MLFLOW_SERVER_PORT}:${MLFLOW_SERVER_PORT}
    volumes:
      - ./data/mlflow:/app
    restart: always
    networks:
      - ollama-docker
  
networks:
  ollama-docker:
    external: false