# https://mlflow.org/docs/latest/llms/gateway/migration.html#gateway-migration
endpoints:
  - name: ollama
    endpoint_type: llm/v1/chat
    model:
      provider: openai
      name: llama3
      config:
        openai_api_key: ""
        # https://ollama.com/blog/openai-compatibility
        openai_api_base: http://host.docker.internal:11434/v1
        
        # TODO: this should not be commented when running on prod
        # openai_api_base: http://llm/v1
